{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"3tl3I8xlYJDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 1: Generate input and output samples for Boolean functions\n","\n","# Define the input samples (X) for AND, OR, and NOR"],"metadata":{"id":"wDPaKuOFYQOp"}},{"cell_type":"code","source":["X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])"],"metadata":{"id":"YeeImXB6YYRv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the output samples (y) for AND, OR, and NOR"],"metadata":{"id":"xJFmWlkaYfyG"}},{"cell_type":"code","source":["y_and = np.array([0, 0, 0, 1])\n","y_or = np.array([0, 1, 1, 1])\n","y_nor = np.array([1, 0, 0, 0])\n"],"metadata":{"id":"sD5GEbyZYkx_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 2 & 3: Implement a single-layer **perceptron** with backpropagation using the gradient descent algorithm."],"metadata":{"id":"__Ob2mZeYt_M"}},{"cell_type":"code","source":["class Perceptron:\n","    def __init__(self, input_size):\n","        self.weights = np.random.rand(input_size)\n","        self.bias = np.random.rand()\n","\n","    def activation(self, x):\n","        return 1 if x >= 0 else 0\n","\n","    def feedforward(self, x):\n","        weighted_sum = np.dot(x, self.weights) + self.bias\n","        return self.activation(weighted_sum)\n","\n","    def train(self, X, y, learning_rate, epochs):\n","        for _ in range(epochs):\n","            for i in range(len(X)):\n","                prediction = self.feedforward(X[i])\n","                error = y[i] - prediction\n","\n","                # Update weights using gradient descent\n","                self.weights += learning_rate * error * X[i]\n","                self.bias += learning_rate * error"],"metadata":{"id":"L7DcjzHsY1DU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 4: Train and test the neural network for AND, OR, and NOR"],"metadata":{"id":"-JmtKfR9OERo"}},{"cell_type":"code","source":["# Define the learning rate and number of training epochs\n","learning_rate = 0.1\n","epochs = 100\n","\n","# Create Perceptron instances for AND, OR, and NOR with 2 input nodes\n","perceptron_and = Perceptron(2)\n","perceptron_or = Perceptron(2)\n","perceptron_nor = Perceptron(2)\n","\n","# Train the perceptrons\n","perceptron_and.train(X, y_and, learning_rate, epochs)\n","perceptron_or.train(X, y_or, learning_rate, epochs)\n","perceptron_nor.train(X, y_nor, learning_rate, epochs)"],"metadata":{"id":"TVj72mFbNnjm","executionInfo":{"status":"ok","timestamp":1694982358844,"user_tz":-60,"elapsed":585,"user":{"displayName":"Oyindamola Olatunji","userId":"00342964722482903533"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Display the results for AND, OR, and NOR"],"metadata":{"id":"gYCOKu-cOiQu"}},{"cell_type":"code","source":["print(\"AND Gate:\")\n","for i in range(len(X)):\n","    print(f\"Input: {X[i]}, Output: {perceptron_and.feedforward(X[i])}\")\n","\n","print(\"\\nOR Gate:\")\n","for i in range(len(X)):\n","    print(f\"Input: {X[i]}, Output: {perceptron_or.feedforward(X[i])}\")\n","\n","print(\"\\nNOR Gate:\")\n","for i in range(len(X)):\n","    print(f\"Input: {X[i]}, Output: {perceptron_nor.feedforward(X[i])}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YxUNYh4kOYXj","executionInfo":{"status":"ok","timestamp":1694982362062,"user_tz":-60,"elapsed":565,"user":{"displayName":"Oyindamola Olatunji","userId":"00342964722482903533"}},"outputId":"faa4fe00-c805-4261-8f72-ce6c3afa2bc4"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["AND Gate:\n","Input: [0 0], Output: 0\n","Input: [0 1], Output: 0\n","Input: [1 0], Output: 0\n","Input: [1 1], Output: 1\n","\n","OR Gate:\n","Input: [0 0], Output: 0\n","Input: [0 1], Output: 1\n","Input: [1 0], Output: 1\n","Input: [1 1], Output: 1\n","\n","NOR Gate:\n","Input: [0 0], Output: 1\n","Input: [0 1], Output: 0\n","Input: [1 0], Output: 0\n","Input: [1 1], Output: 0\n"]}]},{"cell_type":"code","source":["final_weights = perceptron_or.weights\n","print(final_weights)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBbPefcru0I6","executionInfo":{"status":"ok","timestamp":1694982365483,"user_tz":-60,"elapsed":10,"user":{"displayName":"Oyindamola Olatunji","userId":"00342964722482903533"}},"outputId":"8d0bd236-473e-4625-afd6-2d89af0c5922"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.44682042 0.83816691]\n"]}]},{"cell_type":"code","source":["final_bias = perceptron_or.bias\n","print(final_bias)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_s4zv290oyy","executionInfo":{"status":"ok","timestamp":1694982395739,"user_tz":-60,"elapsed":672,"user":{"displayName":"Oyindamola Olatunji","userId":"00342964722482903533"}},"outputId":"26f174bd-7402-4567-d5d3-d548a83a8fcb"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["-0.08829361601965835\n"]}]},{"cell_type":"markdown","source":["# Step 6: Can a single-layer perceptron represent the XOR Boolean function? Why or why not?\n","No, a single-layer perceptron cannot represent the XOR function. A single-layer perceptron is limited to linearly separable problems and cannot capture the XOR function's behavior, which is fundamentally nonlinear and requires a more complex neural network structure to be represented accurately"],"metadata":{"id":"L1ky57VEPAKl"}},{"cell_type":"markdown","source":["# Step 7: Enhance the neural network to accurately represent the XOR Boolean function."],"metadata":{"id":"3pDAuehtQh__"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the XOR input and target output\n","X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","y_xor = np.array([[0], [1], [1], [0]])  # Change y_xor to be a column vector\n","\n","# Define activation function (sigmoid)\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Initialize random weights and biases for the hidden and output layers\n","input_size = 2\n","hidden_size = 2\n","output_size = 1\n","learning_rate = 0.1\n","epochs = 10000  # Increase the number of epochs for better training\n","\n","# Initialize weights and biases with random values\n","weights_input_hidden = np.random.uniform(size=(input_size, hidden_size))\n","bias_hidden = np.random.uniform(size=(1, hidden_size))\n","weights_hidden_output = np.random.uniform(size=(hidden_size, output_size))\n","bias_output = np.random.uniform(size=(1, output_size))\n","\n","# Training the neural network\n","for epoch in range(epochs):\n","    # Forward pass\n","    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n","    hidden_output = sigmoid(hidden_input)\n","    output_layer_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n","    output = sigmoid(output_layer_input)\n","\n","    # Calculate loss\n","    loss = ((y_xor - output) ** 2).mean()\n","\n","    # Backpropagation\n","    d_output = 2 * (output - y_xor) * output * (1 - output)\n","    d_hidden = d_output.dot(weights_hidden_output.T) * hidden_output * (1 - hidden_output)\n","\n","    # Update weights and biases\n","    weights_hidden_output -= hidden_output.T.dot(d_output) * learning_rate\n","    bias_output -= np.sum(d_output, axis=0, keepdims=True) * learning_rate\n","    weights_input_hidden -= X.T.dot(d_hidden) * learning_rate\n","    bias_hidden -= np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n","\n","    # Print loss for every 1000 epochs\n","    if epoch % 1000 == 0:\n","        print(f'Epoch {epoch}, Loss: {loss}')\n","\n","# Print the final predictions\n","print(\"Final Predictions:\")\n","print(output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cR-OPR7EI4rV","executionInfo":{"status":"ok","timestamp":1694980693156,"user_tz":-60,"elapsed":966,"user":{"displayName":"Oyindamola Olatunji","userId":"00342964722482903533"}},"outputId":"c784c9a7-40be-4bc0-f54e-413e91df1bf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.28823701290837767\n","Epoch 1000, Loss: 0.24897225968746817\n","Epoch 2000, Loss: 0.20404355724661905\n","Epoch 3000, Loss: 0.14221153430108607\n","Epoch 4000, Loss: 0.13219319163078003\n","Epoch 5000, Loss: 0.1293100172675941\n","Epoch 6000, Loss: 0.1280235479230164\n","Epoch 7000, Loss: 0.12730995807467027\n","Epoch 8000, Loss: 0.12686065336435892\n","Epoch 9000, Loss: 0.12655342981142914\n","Final Predictions:\n","[[0.02782302]\n"," [0.96522714]\n"," [0.4990985 ]\n"," [0.50243268]]\n"]}]}]}